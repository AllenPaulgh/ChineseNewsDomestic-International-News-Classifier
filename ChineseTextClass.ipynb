{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import codecs\n",
    "from spacy.lang.zh import Chinese\n",
    "from gensim.models import Word2Vec\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('wiki.zh.vec')\n",
    "#scikitlearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = codecs.open('stopwords-zh.txt', 'r', 'utf-8').read().split(',')\n",
    "\n",
    "\n",
    "jieba_stop_words = [\n",
    "    '的', '了', '和', '是', '就', '都', '而', '及', '與', \n",
    "    '著', '或', '一個', '沒有', '我們', '你們', '妳們', \n",
    "    '他們', '她們', '是否', '“','”','‘','’','？','\\n',' ',\n",
    "    '有','这','让','今年','目前','要','%'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zng(paragraph):\n",
    "    for sent in re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?’“”]?', paragraph, flags=re.U):\n",
    "        yield sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model(Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([ ('clf', LogisticRegression(\n",
    "        random_state = 1,\n",
    "        solver = 'saga',\n",
    "        multi_class= 'ovr',\n",
    "        max_iter=10000\n",
    "    ))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = Chinese()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# text = u\"你要去那我也 我 去爸“”“。大哥你今天要去学校吗“”“。\"\n",
    "def text_clean(text):\n",
    "    collect = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text not in jieba_stop_words:\n",
    "            collect.append(token.text)\n",
    "    collect = ''.join(collect)\n",
    "    return collect    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.read_csv(\"chinese_news.csv\", usecols = ['tag','content','headline'], encoding = 'utf-8' )\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(len(corpus_df))\n",
    "corpus_df=corpus_df.dropna(subset=['tag','headline','content'])\n",
    "#print(len(corpus_df))\n",
    "corpus_df['text'] = corpus_df['headline']+\" \"+corpus_df['content']\n",
    "corpus_df.head()\n",
    "corpus_df.drop(columns = ['headline','content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df['text'] = corpus_df['text'].apply(text_clean)\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headline_df=corpus_df[corpus_df['tag']=='详细全文']\n",
    "train_international_df=corpus_df[corpus_df['tag']== '国际' ]\n",
    "train_domestic_df=corpus_df[corpus_df['tag']== '国内' ]\n",
    "print(len(train_headline_df))\n",
    "print(len(train_international_df))\n",
    "print(len(train_domestic_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df = train_headline_df[0:2000]\n",
    "international_df = train_international_df[0:2000]\n",
    "domestic_df = train_domestic_df[0:2000]\n",
    "\n",
    "headline_df.reset_index(drop = True, inplace = True)\n",
    "international_df.reset_index(drop = True, inplace = True)\n",
    "domestic_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "train_df=pd.concat([headline_df,international_df, domestic_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(headline_df))\n",
    "print(len(international_df))\n",
    "print(len(domestic_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_headline_df = train_headline_df[2000:3000]\n",
    "test_international_df = train_international_df[2000:3000]\n",
    "test_domestic_df = train_domestic_df[2000:3000]\n",
    "\n",
    "test_headline_df.reset_index(drop = True, inplace = True)\n",
    "test_international_df.reset_index(drop = True, inplace = True)\n",
    "test_domestic_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "test_df=pd.concat([test_headline_df,test_international_df, test_domestic_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_headline_df))\n",
    "print(len(test_international_df))\n",
    "print(len(test_domestic_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting columns for training\n",
    "X = list(train_df['text'])\n",
    "Y = list(train_df['tag'])\n",
    "X_test = list(test_df['text'])\n",
    "Y_test = list(test_df['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenizer using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zng(paragraph):\n",
    "    for sent in re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?’“”]?', paragraph, flags=re.U):\n",
    "        yield sent\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_vector (Hiearcheal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write get_vector function\n",
    "#needs to take in a string\n",
    "#seperate sentences into lists of strings\n",
    "#vectorize list of strings\n",
    "#take average of all vectors\n",
    "#output stack of vectors as one vector\n",
    "import ast\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.zh import Chinese\n",
    "from spacy.tokens import Doc\n",
    "nlp = Chinese()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "text = u\"我很喜欢广州。广州是我最喜欢的中国城市。三年前，我飞到广州见朋友了。我们一起过中秋节\"\n",
    "def get_vector_ap(text):\n",
    "    doc = nlp(text)\n",
    "    sent_collect = []\n",
    "    sent_vector_collect = []\n",
    "    #get sentences with re\n",
    "    sent_collect = list(zng(text))\n",
    "#     print(list(sent_collect))\n",
    "           \n",
    "    for sent in sent_collect:\n",
    "        word_vector_collect = []\n",
    "#         print(\"sentence acquired:\",sent, '\\n')  \n",
    "        for each_word in sent:   \n",
    "            try:\n",
    "                word_vector = model.wv[each_word]\n",
    "#                 print(\"Size of Vector : {0}\".format(len(word_vector)))                \n",
    "                word_vector_collect.append(word_vector)\n",
    "                            \n",
    "#               print(sent_vector_collect)\n",
    "            except KeyError:\n",
    "#                 print(\"Token {0} NOT FOUND\".format(each_word))\n",
    "                each_word.replace(\"”\",\" \")\n",
    "           \n",
    "        word_vector_np_array = np.asarray(word_vector_collect)\n",
    "        word_vector_mean = np.mean(word_vector_np_array, axis=0)\n",
    "        assert word_vector_mean.shape == (300,), \"I expected a 300 dimensional vector, but received: {0} for -{1}-\".format(word_vector_mean.shape, sent)\n",
    "        sent_vector_collect.append(word_vector_mean)\n",
    "        \n",
    "    sent_vector_np_array = np.asarray(sent_vector_collect)\n",
    "#     print(\"Shape of Sent Vector Numpy Array : {0}\".format(sent_vector_np_array.shape))\n",
    "    \n",
    "    doc_vector = np.mean(sent_vector_np_array, axis=0)\n",
    "#     print(\"Shape of Doc Vector Numpy Array : {0}\".format(doc_vector.shape))\n",
    "    assert doc_vector.shape == (300,), \"I expected a 300 dimensional vector\"\n",
    "    return doc_vector\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val  = train_test_split(X,Y, test_size=0.2, stratify=Y, random_state=882)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(X_val[0])\n",
    "print(Y_train[0])\n",
    "print(Y_val[0])\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_val))\n",
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "print(len(Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Vectors (Training Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Vec_X_train = []\n",
    "\n",
    "for i in X_train:\n",
    "    i = i.strip()\n",
    "    Vec_X_train.append(get_vector_ap(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Vec_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(Vec_X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy_ops Tpkenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spacy_ops_zh(object):\n",
    "    def __init__(self):\n",
    "        self.nlp = Chinese()\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "    def __call__(self, some_text):\n",
    "        doc = self.nlp(some_text)\n",
    "        return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFID CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer(\n",
    "max_features = 10,\n",
    "tokenizer = spacy_ops_zh(),\n",
    "ngram_range = (2,2)\n",
    "\n",
    ")\n",
    "vectors = cv.fit_transform(X_train,Y_train)\n",
    "feature_names = cv.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nFEATURES\\n\\n\")\n",
    "clf_features = feature_names\n",
    "clf_coeffs_headline = text_clf.get_params()['clf'].coef_[2]\n",
    "clf_coeffs_domestic = text_clf.get_params()['clf'].coef_[0]\n",
    "clf_coeffs_international = text_clf.get_params()['clf'].coef_[1]\n",
    "k = 25\n",
    "highest_headline_features = clf_coeffs_headline.argsort()[-k:][::-1]\n",
    "highest_domestic_features = clf_coeffs_domestic.argsort()[-k:][::-1]\n",
    "highest_international_features = clf_coeffs_international.argsort()[-k:][::-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(highest_headline_features)\n",
    "\n",
    "print(highest_domestic_features)\n",
    "\n",
    "print(highest_international_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Vectors (Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vec_X_val = []\n",
    "for i in X_val:\n",
    "    i = i.strip()\n",
    "    Vec_X_val.append(get_vector_ap(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(Vec_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=Y_val, y_pred=predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
